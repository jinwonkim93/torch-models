{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#봉수메모\n",
    "\n",
    "#pytorch.org\n",
    "#1d, 2d, 3d\n",
    "\n",
    "#nn.linear\n",
    "#nn.Conv2d\n",
    "#MAXPOOL\n",
    "\n",
    "\n",
    "#input\n",
    "#224x224x3 rgb image\n",
    "#in channal 3\n",
    "#image_size 224\n",
    "\n",
    "#conv\n",
    "#out channel 64 -> rgb 64\n",
    "#kernel_size 3 -> 이미지 가로 세로 3x3\n",
    "\n",
    "#maxpool\n",
    "#stride 2\n",
    "#filter 2\n",
    "#channel 없음\n",
    "\n",
    "#weight, bias <- prams orint\n",
    "\n",
    "#dummy -> conv1 -> conv2\n",
    "\n",
    "#shit m 셀합치기\n",
    "\n",
    "#채널이 왜 점점 커지는지 -> 채널이 각 픽셀에 대한 피쳐 공간\n",
    "#d 번 세팅 기중\n",
    "\n",
    "#flatten layer 만들면 디파인 가능, 더 빠름\n",
    "\n",
    "#찍어보고해야함\n",
    "#activation 파라미터있음\n",
    "#dense 도 아웃풋만줌\n",
    "\n",
    "#tensorflow\n",
    "#lazy linear\n",
    "#sequential로 묶기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = torch.nn.ReLU(inplace=False)\n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.softmax = torch.nn.Softmax(1)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.relu,\n",
    "            self.conv2,\n",
    "            self.relu,\n",
    "            self.maxpool\n",
    "            )\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            self.conv3,\n",
    "            self.relu,\n",
    "            self.conv4,\n",
    "            self.relu,\n",
    "            self.maxpool\n",
    "            )\n",
    "\n",
    "        self.conv5 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv6 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv7 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            self.conv5,\n",
    "            self.relu,\n",
    "            self.conv6,\n",
    "            self.relu,\n",
    "            self.conv7,\n",
    "            self.relu,\n",
    "            self.maxpool\n",
    "            )\n",
    "\n",
    "        self.conv8 = torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv9 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv10 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.conv8,\n",
    "            self.relu,\n",
    "            self.conv9,\n",
    "            self.relu,\n",
    "            self.conv10,\n",
    "            self.relu,\n",
    "            self.maxpool\n",
    "            )\n",
    "\n",
    "        self.conv11 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv12 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv13 = torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            self.conv11,\n",
    "            self.relu,\n",
    "            self.conv12,\n",
    "            self.relu,\n",
    "            self.conv13,\n",
    "            self.relu,\n",
    "            self.maxpool\n",
    "            )\n",
    "\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.LazyLinear(out_features=4096)\n",
    "        self.fc2 = torch.nn.Linear(in_features=4096, out_features=4096)\n",
    "        self.fc3 = torch.nn.Linear(in_features=4096, out_features=n_classes)\n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            self.flat,\n",
    "            self.fc1,\n",
    "            self.relu,\n",
    "            self.fc2,\n",
    "            self.relu,\n",
    "            self.fc3,\n",
    "            self.softmax\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummpy = torch.zeros((1, 3, 224, 224))\n",
    "# h = relu(conv1(dummpy))\n",
    "# print(h.shape)\n",
    "# h2 = relu(conv2(h))\n",
    "# print(h2.shape)\n",
    "# h3 = maxpool(h2)\n",
    "# print(h3.shape)\n",
    "# h4 = relu(conv3(h3))\n",
    "# print(h4.shape)\n",
    "# h5 = relu(conv4(h4))\n",
    "# print(h5.shape)\n",
    "# h6 = maxpool(h5)\n",
    "# print(h6.shape)\n",
    "\n",
    "# h7 = relu(conv5(h6))\n",
    "# print(h7.shape)\n",
    "# h8 = relu(conv6(h7))\n",
    "# print(h8.shape)\n",
    "# h9 = relu(conv7(h8))\n",
    "# print(h9.shape)\n",
    "# h9 = maxpool(h9)\n",
    "\n",
    "# h10 = relu(conv8(h9))\n",
    "# print(h10.shape)\n",
    "# h11 = relu(conv9(h10))\n",
    "# print(h11.shape)\n",
    "# h12 = relu(conv10(h11))\n",
    "# print(h12.shape)\n",
    "# h12 = maxpool(h12)\n",
    "\n",
    "# h13 = relu(conv11(h12))\n",
    "# print(h13.shape)\n",
    "# h14 = relu(conv12(h13))\n",
    "# print(h14.shape)\n",
    "# h15 = relu(conv13(h14))\n",
    "# print(h15.shape)\n",
    "# h16 = maxpool(h15)\n",
    "# print(h16.shape)\n",
    "# h16 = flat(h16)\n",
    "# print(h16.shape)\n",
    "\n",
    "# h17 = relu(fc1(h16))\n",
    "# print(h17.shape)\n",
    "# h18 = relu(fc2(h17))\n",
    "# print(h18.shape)\n",
    "# h19 = softmax(fc3(h18))\n",
    "# print(h19.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jw93/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "vgg16 = VGG16(n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.zeros((1, 3, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vgg16(dummy)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb 셀 7\u001b[0m in \u001b[0;36mVGG16.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X44sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X44sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X44sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer5(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X44sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer6(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X44sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "vgg16(dummy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(vgg16.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2685, grad_fn=<DivBackward1>)\n",
      "tensor(12.1949, grad_fn=<DivBackward1>)\n",
      "tensor(11.8856, grad_fn=<DivBackward1>)\n",
      "tensor(11.9407, grad_fn=<DivBackward1>)\n",
      "tensor(11.4696, grad_fn=<DivBackward1>)\n",
      "tensor(12.3180, grad_fn=<DivBackward1>)\n",
      "tensor(11.5735, grad_fn=<DivBackward1>)\n",
      "tensor(10.7274, grad_fn=<DivBackward1>)\n",
      "tensor(11.4615, grad_fn=<DivBackward1>)\n",
      "tensor(11.4266, grad_fn=<DivBackward1>)\n",
      "tensor(11.5872, grad_fn=<DivBackward1>)\n",
      "tensor(11.9197, grad_fn=<DivBackward1>)\n",
      "tensor(11.5665, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)        \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw93/Desktop/workspace/vgg/vgg_classification.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        batch = torch.rand((16, 3, 224, 224), dtype=torch.float32)\n",
    "        target = torch.rand(size=(16, 10))\n",
    "        pred = vgg16(batch)\n",
    "        loss = loss_fn(pred, target)\n",
    "        print(loss)        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
